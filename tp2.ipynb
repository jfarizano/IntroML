{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2 - Introducción al aprendizaje automatizado\n",
    "### Juan Ignacio Farizano\n",
    "\n",
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports iniciales\n",
    "%matplotlib inline\n",
    "import math\n",
    "from common import *\n",
    "from ann import *\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores utilizados para crear la red neuronal\n",
    "epochs=20 # numero de epocas que entrena cada vez\n",
    "eta=0.1 # learning rate\n",
    "alfa=0.9 # momentum\n",
    "evals=1000 # evalauciones del entrenamiento\n",
    "\n",
    "col_names = [0, 1, 'Class']\n",
    "x_col_names = col_names[:-1]\n",
    "y_col_name = col_names[-1]\n",
    "\n",
    "# Creo el conjunto de datos utilizado para entrenar y vaidar\n",
    "df_data_spirals = generate_spirals(600)\n",
    "# Separo un 20% al azar para conjunto de validación\n",
    "X, y = df_data_spirals[x_col_names], df_data_spirals[y_col_name]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "\n",
    "# Genero el conjunto de test\n",
    "df_test_spirals = generate_spirals(2000)\n",
    "X_test, y_test = df_test_spirals[x_col_names], df_test_spirals[y_col_name]\n",
    "\n",
    "# Itero para cada cantidad de neuronas en la capa intermedia\n",
    "for N2 in [2, 10, 20, 40]:\n",
    "  # Creo la red neuronal\n",
    "  classif = create_net_classifier(N2, eta, alfa, epochs)\n",
    "\n",
    "  # La entreno con los conjuntos generados anteriormente\n",
    "  best_net, e_train, e_val, e_test = train_net(classif, evals, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "  \n",
    "  # Predigo sobre el conjunto de test\n",
    "  predict_test = best_net.predict(X_test)\n",
    "  df_predict = df_test_spirals.copy(deep = True)\n",
    "  df_predict['Class'] = predict_test\n",
    "\n",
    "  # Grafico los resultados\n",
    "  graph_df(df_predict, \"ANN trained with N2 = {0}\".format(N2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2\n",
    "# Valores utilizados para crear la red neuronal\n",
    "epochs=50 # numero de epocas que entrena cada vez\n",
    "N2=6  # neuronas en la capa oculta\n",
    "evals=300 # evalauciones del entrenamiento\n",
    "etas=[0.1, 0.01, 0.001] # learning rate\n",
    "alfas=[0, 0.5, 0.9] # momentum\n",
    "\n",
    "col_names = [0, 1, 'Class']\n",
    "x_col_names = col_names[:-1]\n",
    "y_col_name = col_names[-1]\n",
    "\n",
    "# Leo el conjunto de datos utilizado para entrenar y vaidar\n",
    "df_data_elipses = pd.read_csv(\"datasets/elipses/dos_elipses.data\", header=None, names = col_names)\n",
    "# Separo un 20% al azar para conjunto de validación\n",
    "X, y = df_data_elipses[x_col_names], df_data_elipses[y_col_name]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "\n",
    "# Leo el conjunto de test\n",
    "df_test_elipses = pd.read_csv(\"datasets/elipses/dos_elipses.test\", header=None, names = col_names)\n",
    "X_test, y_test = df_test_elipses[x_col_names], df_test_elipses[y_col_name]\n",
    "\n",
    "best_mean_test_error = math.inf\n",
    "table = []\n",
    "\n",
    "# Itero para cada combinación de learning rate y momentum\n",
    "for (eta, alfa) in [(e, a) for e in etas for a in alfas]:\n",
    "  train_errors = []\n",
    "  val_errors = []\n",
    "  test_errors = []\n",
    "\n",
    "  # Para cada combinación realizo 10 entrenamientos iguales\n",
    "  for i in range(10):\n",
    "    # Creo la red neuronal\n",
    "    classif = create_net_classifier(N2, eta, alfa, epochs)\n",
    "  \n",
    "    # La entreno con los conjuntos generados anteriormente\n",
    "    best_net, e_train, e_val, e_test = train_net(classif, evals, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "    # Junto todos los errores en listas 2d donde hay una fila por cada una de las\n",
    "    # 10 redes y las columnas son los errores por etapa\n",
    "    train_errors.append(e_train)\n",
    "    val_errors.append(e_val)\n",
    "    test_errors.append(e_test)\n",
    "\n",
    "  train_errors = np.asarray(train_errors)\n",
    "  val_errors = np.asarray(val_errors)\n",
    "  test_errors = np.asarray(test_errors)\n",
    "  \n",
    "  # Calculo los promedios de los errores por cada etapa (columna)\n",
    "  mean_train_errors = train_errors.mean(axis = 0)\n",
    "  mean_val_errors = val_errors.mean(axis = 0)\n",
    "  mean_test_errors = test_errors.mean(axis = 0)\n",
    "\n",
    "  # Busco el mínimo error de validación (y su índice) entre los promedios por cada etapa\n",
    "  min_val_error = np.min(mean_val_errors)\n",
    "  index_min_val_error = np.where(mean_val_errors == min_val_error)[0][0]\n",
    "\n",
    "  # Para la etapa del mínimo error de validación busco su error de test\n",
    "  mean_min_test_error = mean_test_errors[index_min_val_error]\n",
    "\n",
    "  # Si es el mínimo error de test hasta ahora lo guardo junto a su eta, alfa\n",
    "  # y datos de errores correspondientes\n",
    "  if mean_min_test_error < best_mean_test_error:\n",
    "    best_mean_test_error = mean_min_test_error\n",
    "    best_eta = eta\n",
    "    best_alfa = alfa\n",
    "    # Guardo estos errores para graficarlos luego\n",
    "    best_train_errors = np.copy(mean_train_errors)\n",
    "    best_val_errors = np.copy(mean_val_errors)\n",
    "    best_test_errors = np.copy(mean_test_errors)\n",
    "\n",
    "  # Guardo la combinación eta, alfa con su error de test calculado antes\n",
    "  table.append([eta, alfa, mean_min_test_error])\n",
    "\n",
    "errors = []\n",
    "\n",
    "for i in range(evals):\n",
    "  errors.append([best_train_errors[i], i * epochs, \"Train error\"])\n",
    "  errors.append([best_val_errors[i], i * epochs, \"Validation error\"])\n",
    "  errors.append([best_test_errors[i], i * epochs, \"Test error\"])\n",
    "\n",
    "ej3_errors_df = pd.DataFrame(errors, columns = [\"Error\", \"Epocas\", \"Class\"])\n",
    "ej3_errors_df.to_csv(\"datasets/etas_alfas/ann_etas_alfas_errors.csv\", index = False)\n",
    "\n",
    "ej3_errors_table_df = pd.DataFrame(table, columns = [\"Learning rate\", \"Momentum\", \"Mean test error\"])\n",
    "ej3_errors_table_df.to_csv(\"datasets/etas_alfas/ann_etas_alfas_table.csv\", index = False)\n",
    "print(\"Mejor eta: {0} mejor alfa: {1} with test de error promedio: {2}\".format(best_eta, best_alfa, best_mean_test_error))\n",
    "ej3_errors_table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best eta: 0.01 best alfa: 0.9 with mean test error: 0.035699999999999996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df = pd.read_csv(\"datasets/etas_alfas/ann_etas_alfas_errors.csv\")\n",
    "graph_errors(errors_df, 'Epocas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 3\n",
    "# Valores utilizados para crear la red neuronal\n",
    "epochs=50 # numero de epocas que entrena cada vez\n",
    "N2=30  # neuronas en la capa oculta\n",
    "evals=400 # evalauciones del entrenamiento\n",
    "eta=0.01 # learning rate\n",
    "alfa=0.9 # momentum\n",
    "\n",
    "col_names = list(range(5)) + ['Value']\n",
    "x_col_names = col_names[:-1]\n",
    "y_col_name = col_names[-1]\n",
    "\n",
    "# Leo el conjunto de datos utilizado para entrenar y vaidar\n",
    "df_data_ikeda = pd.read_csv(\"datasets/ikeda/ikeda.data\", header=None, names = col_names, delim_whitespace=True, skipinitialspace=True)\n",
    "# Leo el conjunto de test\n",
    "df_test_ikeda = pd.read_csv(\"datasets/ikeda/ikeda.test\", header=None, names = col_names, delim_whitespace=True, skipinitialspace=True)\n",
    "X_test, y_test = df_test_ikeda[x_col_names], df_test_ikeda[y_col_name]\n",
    "\n",
    "for val_size in [0.05, 0.25, 0.5]:\n",
    "  # Separo al azar para conjunto de validación\n",
    "  X, y = df_data_ikeda[x_col_names], df_data_ikeda[y_col_name]\n",
    "  X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=val_size)\n",
    "\n",
    "  # Creo la red neuronal\n",
    "  regr = create_net_regressor(N2, eta, alfa, epochs)\n",
    "  # La entreno con los conjuntos generados anteriormente\n",
    "  best_net, e_train, e_val, e_test = train_net(regr, evals, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "  \n",
    "  fig, ax = plt.subplots(figsize=(15, 10))\n",
    "  ax.grid(which='both', color='grey', linewidth=1, linestyle='-', alpha=0.2)\n",
    "  xrange = [i * epochs for i in range(evals)]\n",
    "  ax.set(xlim=(0, max(xrange)), ylim=(0, 0.6))\n",
    "  plt.plot(xrange, e_train, label=\"Train\", linestyle=\":\")\n",
    "  plt.plot(xrange, e_val, label=\"Validation\", linestyle=\"-.\")\n",
    "  plt.plot(xrange, e_test, label=\"Test\", linestyle=\"-\")\n",
    "  plt.xlabel('Épocas', size=14, labelpad=15)\n",
    "  plt.ylabel('Error', size=14, labelpad=15)\n",
    "  plt.title(\"{0}% train - {1}% validation\".format(int((1 - val_size) * 100), int(val_size * 100)), size = 14, pad = 10)\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Ejercicio 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 4\n",
    "# Valores utilizados para crear la red neuronal\n",
    "epochs=20 # numero de epocas que entrena cada vez\n",
    "N2=6  # neuronas en la capa oculta\n",
    "evals=4000 # evalauciones del entrenamiento\n",
    "eta=0.05 # learning rate\n",
    "alfa=0.3 # momentum\n",
    "\n",
    "\n",
    "col_names=list(range(12)) + ['Value']\n",
    "x_col_names = col_names[:-1]\n",
    "y_col_name = col_names[-1]\n",
    "\n",
    "# Leo el conjunto de datos utilizado para entrenar y vaidar\n",
    "df_train_ssp = pd.read_csv(\"datasets/ssp/ssp.data\", header=None, names = col_names)\n",
    "# Leo el conjunto de test\n",
    "df_test_ssp = pd.read_csv(\"datasets/ssp/ssp.test\", header=None, names = col_names)\n",
    "\n",
    "# Separo los conjuntos de entrenamiento, validación y test en conjuntos\n",
    "# de datos de entrada y con las respuestas.\n",
    "X_train, y_train = df_train_ssp[x_col_names], df_train_ssp[y_col_name]\n",
    "X_test, y_test = df_test_ssp[x_col_names], df_test_ssp[y_col_name]\n",
    "\n",
    "gammas = [10 ** i for i in range(-6, 1)] # weight-decays\n",
    "\n",
    "table = []\n",
    "best_error_test = math.inf\n",
    "\n",
    "for gamma in gammas:\n",
    "  # Creo la red neuronal\n",
    "  regr = create_net_regressor(N2, eta, alfa, epochs, gamma)\n",
    "\n",
    "  # La entreno con los conjuntos generados anteriormente\n",
    "  errors = []\n",
    "  weights = []\n",
    "\n",
    "  for i in range(evals):\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    squared_sum_weights = sum(map(lambda a : np.sum(np.power(a, 2)), regr.coefs_))\n",
    "\n",
    "    predict_train = regr.predict(X_train)\n",
    "    predict_test = regr.predict(X_test)\n",
    "\n",
    "    actual_error_train = sk.metrics.mean_squared_error(y_train, predict_train)\n",
    "    actual_error_test = sk.metrics.mean_squared_error(y_test, predict_test)  \n",
    "\n",
    "    errors.append([actual_error_train, i * epochs, \"Train error\"])\n",
    "    errors.append([actual_error_test, i * epochs, \"Test error\"])\n",
    "    weights.append([i * epochs, squared_sum_weights])\n",
    "\n",
    "  table.append([gamma, actual_error_train, actual_error_test])\n",
    "  \n",
    "  if actual_error_test < best_error_test:\n",
    "    best_error_test = actual_error_test\n",
    "    best_gamma = gamma\n",
    "    best_sum_weight = squared_sum_weights\n",
    "    best_errors_df = pd.DataFrame(errors, columns = [\"Error\", \"Epocas\", \"Class\"])\n",
    "    best_weights_df = pd.DataFrame(weights, columns = [\"Epocas\", \"Pesos\"])\n",
    "\n",
    "ej4_errors_table_df = pd.DataFrame(table, columns = [\"Gamma\", \"Train error\", \"Test error\"])\n",
    "ej4_errors_table_df.to_csv(\"datasets/gammas/ann_gammas_table.csv\", index = False)\n",
    "best_errors_df.to_csv(\"datasets/gammas/ann_gammas_errors.csv\", index = False)\n",
    "best_weights_df.to_csv(\"datasets/gammas/ann_gammas_weights.csv\", index = False)\n",
    "print(\"Mejor gamma: {0} con error de test: {1} y suma de pesos al cuadrado: {2}\".format(best_gamma, best_error_test, best_sum_weight))\n",
    "ej4_errors_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df = pd.read_csv(\"datasets/gammas/ann_gammas_errors.csv\")\n",
    "weights_df = pd.read_csv(\"datasets/gammas/ann_gammas_weights.csv\")\n",
    "\n",
    "graph_errors(errors_df, 'Epocas')\n",
    "graph_weights(weights_df, 'Epocas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Ejercicio 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 5\n",
    "# Valores utilizados para crear la red neuronal\n",
    "epochs=50 # numero de epocas que entrena cada vez\n",
    "N2=6  # neuronas en la capa oculta\n",
    "evals=400 # evalauciones del entrenamiento\n",
    "eta=0.01 # learning rate\n",
    "alfa=0.9 # momentum\n",
    "gamma=10 ** -5 # weight-decay\n",
    "\n",
    "errors = []\n",
    "\n",
    "for d in [2 ** i for i in range(1, 6)]:\n",
    "  x_col_names = list(range(d))\n",
    "  y_col_name = 'Class'\n",
    "\n",
    "  # Genero los conjuntos de test para cada d\n",
    "  df_test_diag = generate_diagonal(d, 10000, 0.78)\n",
    "  df_test_para = generate_parallel(d, 10000, 0.78)\n",
    "\n",
    "  X_test_diag, y_test_diag = df_test_diag[x_col_names], df_test_diag[y_col_name]\n",
    "  X_test_para, y_test_para = df_test_para[x_col_names], df_test_para[y_col_name]\n",
    "\n",
    "  for i in range(20):\n",
    "    # Genero los conjuntos de entrenamiento\n",
    "    df_train_diag = generate_diagonal(d, 250, 0.78)\n",
    "    df_train_para = generate_parallel(d, 250, 0.78)\n",
    "\n",
    "    X_train_diag, y_train_diag = df_train_diag[x_col_names], df_train_diag[y_col_name]\n",
    "    X_train_para, y_train_para = df_train_para[x_col_names], df_train_para[y_col_name]\n",
    "\n",
    "    # Creo las redes neuronales\n",
    "    classif_diag = create_net_classifier(N2, eta, alfa, epochs, gamma)\n",
    "    classif_para = create_net_classifier(N2, eta, alfa, epochs, gamma)\n",
    "\n",
    "    for i in range(evals):\n",
    "      classif_diag.fit(X_train_diag, y_train_diag)\n",
    "      classif_para.fit(X_train_para, y_train_para)\n",
    "\n",
    "    predict_train_diag = classif_diag.predict(X_train_diag)\n",
    "    predict_test_diag = classif_diag.predict(X_test_diag)\n",
    "    predict_train_para = classif_para.predict(X_train_para)\n",
    "    predict_test_para = classif_para.predict(X_test_para)\n",
    "\n",
    "    actual_error_train_diag = sk.metrics.zero_one_loss(y_train_diag, predict_train_diag)\n",
    "    actual_error_test_diag = sk.metrics.zero_one_loss(y_test_diag, predict_test_diag)  \n",
    "    actual_error_train_para = sk.metrics.zero_one_loss(y_train_para, predict_train_para)\n",
    "    actual_error_test_para = sk.metrics.zero_one_loss(y_test_para, predict_test_para)\n",
    "\n",
    "    errors.append([actual_error_train_diag, d, \"ANN - Train diagonal\"])\n",
    "    errors.append([actual_error_test_diag, d, \"ANN - Test diagonal\"])\n",
    "    errors.append([actual_error_train_para, d, \"ANN - Train parallel\"])\n",
    "    errors.append([actual_error_test_para, d, \"ANN - Test parallel\"])\n",
    "\n",
    "ej5_errors_ann_df = pd.DataFrame(errors, columns = [\"Error\", \"d\", \"Class\"])\n",
    "ej5_errors_ann_df.to_csv(\"datasets/dimensions/ann_dimensions_errors.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ej5_errors_ann_df = pd.read_csv(\"datasets/dimensions/ann_dimensions_errors.csv\")\n",
    "ej5_errors_tree_df = pd.read_csv(\"datasets/dimensions/tree_dimensions_errors.csv\")\n",
    "ej5_errors_df = pd.concat([ej5_errors_ann_df, ej5_errors_tree_df])\n",
    "graph_errors(ej5_errors_df, \"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Ejercicio 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ejercicio 4\n",
    "\n",
    "# from sklearn.datasets import load_iris\n",
    "# iris = load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# epochs=50 # numero de epocas que entrena cada vez\n",
    "# N2=6  # neuronas en la capa oculta\n",
    "# evals=400 # evalauciones del entrenamiento\n",
    "# eta=0.01 # learning rate\n",
    "# alfa=0.9 # momentum\n",
    "# gamma=10 ** -5 # weight-decay\n",
    "\n",
    "# col_names=list(range(12)) + ['Value']\n",
    "# x_col_names = col_names[:-1]\n",
    "# y_col_name = col_names[-1]\n",
    "\n",
    "# # Leo el conjunto de datos utilizado para entrenar y vaidar\n",
    "# df_train_faces = pd.read_csv(\"datasets/ssp/ssp.data\", header=None, names = col_names)\n",
    "# # Leo el conjunto de test\n",
    "# df_test_faces = pd.read_csv(\"datasets/ssp/ssp.test\", header=None, names = col_names)\n",
    "\n",
    "# # Separo los conjuntos de entrenamiento, validación y test en conjuntos\n",
    "# # de datos de entrada y con las respuestas.\n",
    "# X_train, y_train = df_train_faces[x_col_names], df_train_faces[y_col_name]\n",
    "# X_test, y_test = df_test_faces[x_col_names], df_test_faces[y_col_name]\n",
    "\n",
    "# gammas = [10 ** i for i in range(-6, 1)] # weight-decays\n",
    "\n",
    "# table = []\n",
    "# best_error_test = math.inf\n",
    "\n",
    "# for gamma in gammas:\n",
    "#   # Creo la red neuronal\n",
    "#   regr = create_net_regressor(N2, eta, alfa, epochs, gamma)\n",
    "\n",
    "#   # La entreno con los conjuntos generados anteriormente\n",
    "#   errors = []\n",
    "#   weights = []\n",
    "\n",
    "#   for i in range(evals):\n",
    "#     regr.fit(X_train, y_train)\n",
    "\n",
    "#     squared_sum_weights = sum(map(lambda a : np.sum(np.power(a, 2)), regr.coefs_))\n",
    "\n",
    "#     predict_train = regr.predict(X_train)\n",
    "#     predict_test = regr.predict(X_test)\n",
    "\n",
    "#     actual_error_train = sk.metrics.mean_squared_error(y_train, predict_train)\n",
    "#     actual_error_test = sk.metrics.mean_squared_error(y_test, predict_test)  \n",
    "\n",
    "#     errors.append([actual_error_train, i * epochs, \"Train error\"])\n",
    "#     errors.append([actual_error_test, i * epochs, \"Test error\"])\n",
    "#     weights.append([i * epochs, squared_sum_weights])\n",
    "\n",
    "#   table.append([gamma, actual_error_train, actual_error_test])\n",
    "  \n",
    "#   if actual_error_test < best_error_test:\n",
    "#     best_error_test = actual_error_test\n",
    "#     best_gamma = gamma\n",
    "#     best_sum_weight = squared_sum_weights\n",
    "#     best_errors_df = pd.DataFrame(errors, columns = [\"Error\", \"Epocas\", \"Class\"])\n",
    "#     best_weights_df = pd.DataFrame(weights, columns = [\"Epocas\", \"Pesos\"])\n",
    "\n",
    "# ej4_errors_table_df = pd.DataFrame(table, columns = [\"Gamma\", \"Train error\", \"Test error\"])\n",
    "# ej4_errors_table_df.to_csv(\"datasets/gammas/ann_gammas_table.csv\", index = False)\n",
    "# best_errors_df.to_csv(\"datasets/gammas/ann_gammas_errors.csv\", index = False)\n",
    "# best_weights_df.to_csv(\"datasets/gammas/ann_gammas_weights.csv\", index = False)\n",
    "# print(\"Mejor gamma: {0} con error de test: {1} y suma de pesos al cuadrado: {2}\".format(best_gamma, best_error_test, best_sum_weight))\n",
    "# ej4_errors_table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Ejercicio 6 bis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a5a52167880705a0cfdf1d0cb75e9380dd218a768d06d94d7eb0bbf7647658a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pypy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
